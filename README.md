## Домашня робота до тем 9 та 10  
  
Файл **run_test_01.py** - тест для завдання 1 (``-h for help``).      
Файл **run_test_02.py** - тест для задачі 2.  
  
  
### **Завдання 1. Висновки.**  
  
#### **Жадібний алгоритм Greedy**  
* Час: **O(k)** (k — кількість номіналів), незалежно від суми `amount`  
* Пам’ять: **O(1)**  
*  Дуже швидкий і масштабований для великих сум  
*  Оптимальний **лише** для “канонічних” наборів монет (типові валютні системи з канонічними номіналами)  
  
#### **Динамічне програмування DP (мінімум монет)**  
* Час: **O(k · amount)** — лінійно зростає зі значенням суми  
* Пам’ять: **O(amount)**  
* Завжди знаходить оптимум, але повільніша і потребує більше пам'яті, особливо для великих `amount`    
  
#### Чому Greedy швидкий на великих сумах?  
Greedy просто ділить `amount` на найбільший номінал, віднімає, переходить до наступного — число операцій обмежене кількістю номіналів `k`. Навіть для `amount = 10^9` час залишається мізерним.  
  
#### Чому DP повільніший на великих сумах?  
Класичний DP заповнює масив довжини `amount`, перевіряючи всі `k` номіналів для кожного значення — отже, час і пам’ять масштабуються з `amount`. Для сотень тисяч/мільйонів це стає відчутно.

**Касовий апарат з типовими номіналами валюти** → **Greedy**: швидкий і (для канонічних номіналів) оптимальний.  
**Довільні/нестандартні номінали** (ваучери, бали, нестандартні купюри) або коли потрібна **строга оптимальність** → **DP**.  
  
  
### **Завдання 2. Висновки.**  

**Монте-Карло** має **середньоквадратичну похибку ~ O(1/√N)**, де N - кількість випадкових точок у **одному експерименті Монте-Карло** (тобто розмір вибірки).  
Це означає, що щоб виграти в одну значущу цифру, N треба збільшити в ~10 разів.  
  
Якщо ми робимо **M незалежних запусків** і усереднюємо їх, то дисперсія середнього зменшується ще у **√M** разів.  

**Отже:**  
* Збільшення **N** у 10 разів (більше точок у кожному запуску) зменшує похибку приблизно у √10 ≈ 3.16 раза.  
* Збільшення **M** у 10 разів (більше незалежних запусків з тим самим N) зменшує похибку **усередненого результату** теж у √10 раза.  
  
**Суттєве зауваження:**  
* Якщо **N маленьке**, то кожен окремий запуск може давати дуже **"шумний"** результат, і усереднення по M запусків згладжує цей шум.  
* Якщо **N велике**, зазвичай достатньо одного експерименту, і повтори дають мало додаткової користі.  
  
